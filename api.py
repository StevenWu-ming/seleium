# æä¾› FastAPI æœå‹™ï¼Œæ”¯æ´è‡ªå‹•åŒ–æ¸¬è©¦åŸ·è¡Œã€ç’°å¢ƒèˆ‡å•†æˆ¶åˆ‡æ›ã€æ¸¬è©¦çµæœæŸ¥è©¢åŠå­˜æ¬¾æµç¨‹é‹è¡Œ
import os
import signal
import subprocess
from multiprocessing import Process, Manager, cpu_count
import time
import threading
import logging
import sys
import unittest
import importlib
from fastapi import FastAPI, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn
from config.config import Config
from utils.test_utils import CleanTextTestResult, CustomTextTestRunner
from playwright.async_api import async_playwright
from pathlib import Path
import re


sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "testbackend")))
from runner_wrapper import run_full_flow

BASE_TEST_DIR = os.path.join(os.path.dirname(__file__), "tests")

app = FastAPI()

class RunParams(BaseModel):
    userName: str
    user_name: str
    password: str
    amount: float

# app.mount("/static", StaticFiles(directory="static", html=True), name="static")
app.mount("/screenshots", StaticFiles(directory="/Users/steven/deepseek/seleium/screenshots"), name="screenshots")

origins = ["http://localhost", "http://127.0.0.1:8000", "*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

log_dir = os.path.dirname(__file__)
log_file = os.path.join(log_dir, 'test_log.log')
try:
    from concurrent_log_handler import ConcurrentRotatingFileHandler
    log_handler = ConcurrentRotatingFileHandler(log_file, mode='a', maxBytes=10*1024*1024, backupCount=1, encoding='utf-8')
except ImportError:
    from logging.handlers import RotatingFileHandler
    log_handler = RotatingFileHandler(log_file, mode='a', maxBytes=10*1024*1024, backupCount=1, encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(name)s] %(message)s",
    handlers=[log_handler, logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("test_logger")

uvicorn_logger = logging.getLogger("uvicorn")
uvicorn_logger.handlers = logger.handlers
uvicorn_logger.setLevel(logging.INFO)

api_test_results = {"status": "not_started", "data": None}

def discover_test_classes():
    discovered = []
    for filename in os.listdir(BASE_TEST_DIR):
        if filename.startswith("test_") and filename.endswith(".py"):
            module_name = filename[:-3]
            module = importlib.import_module(f"tests.{module_name}")
            for attr in dir(module):
                obj = getattr(module, attr)
                if (isinstance(obj, type) and 
                    issubclass(obj, unittest.TestCase) and 
                    obj.__name__ != "BaseTest"):  # æ’é™¤ BaseTest
                    suite = unittest.TestSuite()
                    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(obj))
                    test_count = suite.countTestCases()
                    if test_count > 0:  # åªæ·»åŠ æœ‰æ¸¬è©¦ç”¨ä¾‹çš„é¡
                        discovered.append(obj)
                        logger.info(f"æ¸¬è©¦é¡ {obj.__name__} åŒ…å« {test_count} æ¢æ¸¬è©¦ç”¨ä¾‹")
    return discovered

def run_test_in_process(test_class, result_dict, env, merchant):

    Config.ENV = env
    Config.MERCHANT = merchant
    config = Config.get_current_config()
    process_id = os.getpid()
    logger.info(f"é–‹å§‹é‹è¡Œæ¸¬è©¦é¡ {test_class.__name__}ï¼Œé€²ç¨‹ ID: {process_id}")
    suite = unittest.TestSuite()
    suite.addTest(unittest.TestLoader().loadTestsFromTestCase(test_class))
    runner = CustomTextTestRunner(resultclass=CleanTextTestResult, verbosity=0)
    result = runner.run(suite)
    test_results = result.get_results()
    logger.info(f"æ¸¬è©¦é¡ {test_class.__name__} çµæœ: {test_results}")
    result_dict[test_class.__name__] = test_results

def run_tests_background():
    global api_test_results
    api_test_results["status"] = "running"
    start_time = time.time()

    try:
        config = Config.get_current_config()
        logger.info(f"é–‹å§‹é‹è¡Œæ¸¬è©¦ï¼Œç•¶å‰ç’°å¢ƒ: {config.BASE_URL}ï¼Œå•†æˆ¶: {Config.MERCHANT}")

        config.INVALID_USERNAME_PREFIX = Config.generate_random_username()
        config.INVALID_PHONE_NUMBER = Config.generate_japanese_phone_number()
        config.INVALID_EMAIL = Config.generate_random_email()

        # test_classes = discover_test_classes()
        # with Manager() as manager:
        #     shared_results = manager.dict()
        #     max_workers = cpu_count()
        #     active_processes = []

        #     for test_class in test_classes:
        #         while len(active_processes) >= max_workers:
        #             for p in active_processes:
        #                 if not p.is_alive():
        #                     active_processes.remove(p)
        #             time.sleep(0.2)

        #         # p = Process(target=run_test_in_process, args=(test_class, shared_results))
        #         p = Process(target=run_test_in_process, args=(test_class, shared_results, Config.ENV, Config.MERCHANT))

        #         p.start()
        #         active_processes.append(p)

        #     for p in active_processes:
        #         p.join()

        #     pass_count = 0
        #     fail_count = 0
        #     passed_tests = []
        #     failed_tests = []

        #     for test_class_name, result in shared_results.items():
        #         summary = result.get("summary", {})
        #         pass_count += summary.get("pass_count", 0)
        #         fail_count += summary.get("fail_count", 0)
        #         passed_tests.extend(result.get("passed_tests", []))
        #         failed_tests.extend(result.get("failed_tests", []))

        test_classes = discover_test_classes()
        with Manager() as manager:
            shared_results = manager.dict()
            max_workers = cpu_count()
            active_processes = []

            for test_class in test_classes:
                while len(active_processes) >= max_workers:
                    for p in active_processes:
                        if not p.is_alive():
                            active_processes.remove(p)
                    time.sleep(0.2)

                p = Process(target=run_test_in_process, args=(test_class, shared_results, Config.ENV, Config.MERCHANT))
                p.start()
                active_processes.append(p)

            for p in active_processes:
                p.join()

            # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰æ¸¬è©¦é¡éƒ½æœ‰çµæœ
            missing_classes = [cls.__name__ for cls in test_classes if cls.__name__ not in shared_results]
            if missing_classes:
                logger.error(f"ä»¥ä¸‹æ¸¬è©¦é¡çµæœéºæ¼: {missing_classes}")
            else:
                logger.info(f"æ‰€æœ‰æ¸¬è©¦é¡çµæœå·²æ”¶é›†: {list(shared_results.keys())}")

            pass_count = 0
            fail_count = 0
            passed_tests = []
            failed_tests = []

            for test_class_name, result in shared_results.items():
                summary = result.get("summary", {})
                pass_count += summary.get("pass_count", 0)
                fail_count += summary.get("fail_count", 0)
                passed_tests.extend(result.get("passed_tests", []))
                failed_tests.extend(result.get("failed_tests", []))


            run_time = time.time() - start_time
            api_test_results["status"] = "completed"
            api_test_results["data"] = {
                "summary": {"pass_count": pass_count, "fail_count": fail_count, "total_count": pass_count + fail_count},
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "run_time": f"{run_time:.2f} ç§’"
            }
        logger.info("âœ… æ¸¬è©¦åŸ·è¡Œå®Œæˆ")
    except Exception as e:
        api_test_results["status"] = "failed"
        api_test_results["data"] = {"error": str(e)}
        logger.exception("âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—")

@app.post("/run-tests")
async def run_tests():
    global api_test_results
    if api_test_results["status"] == "running":
        return {"status": "still_running"}
    thread = threading.Thread(target=run_tests_background)
    thread.start()
    return {"status": "started"}

@app.get("/test-results")
async def get_test_results():
    return api_test_results

@app.post("/set-env")
async def set_env(env: str = Body(..., embed=True)):
    valid_envs = ["TestEnv", "ProdEnv", "DevEnv"]
    if env not in valid_envs:
        raise HTTPException(status_code=400, detail=f"ç„¡æ•ˆçš„ç’°å¢ƒåç¨±ï¼Œè«‹ä½¿ç”¨: {valid_envs}")
    Config.ENV = env
    logger.info(f"âœ… ç’°å¢ƒè®Šæ›´ç‚º: {Config.ENV}")
    return {"message": f"æˆåŠŸåˆ‡æ›åˆ°ç’°å¢ƒ: {env}"}

@app.post("/set-merchant")
async def set_merchant(merchant: str = Body(..., embed=True)):
    valid_merchants = ["Merchant1", "Merchant2", "Merchant5", "Merchant7"]
    if merchant not in valid_merchants:
        raise HTTPException(status_code=400, detail=f"ç„¡æ•ˆçš„å•†æˆ¶ï¼Œè«‹ä½¿ç”¨: {valid_merchants}")
    Config.MERCHANT = merchant
    logger.info(f"âœ… å•†æˆ¶è®Šæ›´ç‚º: {Config.MERCHANT}")
    return {"message": f"æˆåŠŸåˆ‡æ›åˆ°å•†æˆ¶: {merchant}"}

@app.post("/run-login_deposit")
def run_all(params: RunParams):
    try:
        run_full_flow(userName=params.userName, user_name=params.user_name, password=params.password, amount=params.amount)
        return {"message": "âœ… è‡ªå‹•åŒ–å°å·¥å…·-å­˜æ¬¾æµç¨‹å®Œæˆ"}
    except Exception as e:
        return {"error": "â" f"åŸ·è¡Œå¤±æ•—: {str(e)}"}


@app.post("/run-sports-screenshot")
async def run_sports_screenshot():
    result = subprocess.run(
        ["python", "tests/sports-screenshot.py"],
        capture_output=True,
        text=True,
        timeout=300
    )

    output = result.stdout + "\n" + result.stderr  # çµåˆ stdout å’Œ stderrï¼ˆæ–·è¨€éŒ¯èª¤æœƒå‡ºç¾åœ¨ stderrï¼‰

    # âœ… è§£ææˆåŠŸèˆ‡å¤±æ•—
    success_matches = re.findall(r"ğŸ“¸ å·²å„²å­˜ (\w+) ç•«é¢", output)
    failed_matches = re.findall(r"âŒ æ‰¾ä¸åˆ° (\w+) çš„ iframe", output)

    # âœ… çµ±ä¸€å›å‚³æ ¼å¼ï¼ˆä¸è«– returncodeï¼‰
    return JSONResponse(content={
        "message": "æ“·å–å®Œæˆ",
        "success": success_matches,
        "failed": failed_matches,
        "count": {
            "total": len(success_matches) + len(failed_matches),
            "success": len(success_matches),
            "fail": len(failed_matches)
        },
    })

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_config=None)
